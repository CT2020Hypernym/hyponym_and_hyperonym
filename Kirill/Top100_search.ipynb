{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopN_search.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBlhFf7_nxfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sivW43H3n70h",
        "colab_type": "code",
        "outputId": "50a8f01d-ec0f-4b37-9c59-f7e922d50362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip install annoy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/15/5a9db225ebda93a235aebd5e42bbf83ab7035e7e4783c6cb528c635c9afb/annoy-1.16.3.tar.gz (644kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 2.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.16.3-cp36-cp36m-linux_x86_64.whl size=297342 sha256=873538073a0b4451f78e7b29b9d6f20556e2741a384cbe504dda0e661251d42e\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/01/54/6ef760fe9f9fc6ba8c19cebbe6358212b5f3b5b0195c0b813f\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.16.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVAmoz6HoEsu",
        "colab_type": "code",
        "outputId": "033844b2-b2bc-42c2-b47a-ee138c686deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlsM0iQPn642",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Data processing: test_set and ruwordnet vectors\n",
        "\n",
        "df_nouns = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fasttext vectors/nouns_private.csv')\n",
        "df_ruwordnet = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fasttext vectors/ruWordNet_vectors.csv')\n",
        "\n",
        "nouns_names = df_nouns['Word'].to_list()\n",
        "nouns_vectors = df_nouns['Vector'].to_list()\n",
        "\n",
        "ruwordnet_names = df_ruwordnet['Word'].to_list()\n",
        "ruwordnet_vectors = df_ruwordnet['Vector'].to_list()\n",
        "\n",
        "import ast\n",
        "\n",
        "for i in range(len(nouns_vectors)):\n",
        "    nouns_vectors[i] = ast.literal_eval(nouns_vectors[i])\n",
        "\n",
        "for i in range(len(ruwordnet_vectors)):\n",
        "    ruwordnet_vectors[i] = ast.literal_eval(ruwordnet_vectors[i])\n",
        "\n",
        "nouns_dict = dict(zip(nouns_names, nouns_vectors))\n",
        "ruwordnet_dict = dict(zip(ruwordnet_names, ruwordnet_vectors))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJLHvWHTzlWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Data processing: train dataset\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fasttext vectors/training_nouns.tsv', sep='\\t')\n",
        "\n",
        "train_hyponyms = df_train['TEXT'].str.lower().to_list()\n",
        "train_hypernyms = df_train['PARENT_TEXTS'].str.lower().to_list()\n",
        "\n",
        "for i in range(len(train_hypernyms)):\n",
        "    train_hypernyms[i] = ast.literal_eval(train_hypernyms[i])\n",
        "    new_list = list()\n",
        "    for j in range(len(train_hypernyms[i])):\n",
        "        string_ = train_hypernyms[i][j]\n",
        "        list_ = string_.split(',')\n",
        "        for k in range(len(list_)):\n",
        "            new_list.append(list_[k])\n",
        "    train_hypernyms[i] = new_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD5N1hhCBETq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title FastText model for new words\n",
        "\n",
        "from gensim.models.wrappers import FastText\n",
        "model = FastText.load_fasttext_format('/content/drive/My Drive/Colab Notebooks/ft_native_300_ru_wiki_lenta_lower_case.bin')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq08FYBqoYYV",
        "colab_type": "code",
        "outputId": "10389ef5-766c-49ad-c2c2-72605b218a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# @title Build AnnoyIndex\n",
        "\n",
        "from annoy import AnnoyIndex\n",
        "import random\n",
        "\n",
        "f = 300\n",
        "t = AnnoyIndex(f, 'angular')  # Length of item vector that will be indexed\n",
        "for i in range(len(ruwordnet_names)):\n",
        "    v = ruwordnet_vectors[i]\n",
        "    t.add_item(i, v)\n",
        "\n",
        "t.build(10) # (number) of trees"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmPMcK_0yC6V",
        "colab_type": "code",
        "outputId": "fb86d0ca-9bcc-4dbd-a618-87a309499060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# @title Count true hypernymms in topN for train_set\n",
        "\n",
        "count_true = np.zeros(len(train_hyponyms))\n",
        "\n",
        "for i in range(len(train_hyponyms)):\n",
        "    hyponim = train_hyponyms[i]\n",
        "    hyponim = hyponim.split(',')[0]\n",
        "    hypernyms = train_hypernyms[i]\n",
        "    v = model[hyponim]\n",
        "    idxs = t.get_nns_by_vector(v, 400)\n",
        "    words = [ruwordnet_names[j] for j in idxs]\n",
        "    for w in words:\n",
        "        for h in hypernyms:\n",
        "            if w == h:\n",
        "                count_true[i] += 1\n",
        "\n",
        "print(np.where(count_true>0)[0].shape[0])\n",
        "print(count_true.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1407\n",
            "17242\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuubXtbb_M32",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Save to csv\n",
        "\n",
        "top100_dict = dict()\n",
        "\n",
        "for noun in nouns_names:\n",
        "    v = nouns_dict[noun]\n",
        "    idxs = t.get_nns_by_vector(v, 200)\n",
        "    words = [ruwordnet_names[i] for i in idxs]\n",
        "    top100_dict[noun] = words\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('top100.csv', 'w') as f:\n",
        "    for key in top100_dict.keys():\n",
        "        f.write(\"%s,%s\\n\"%(key,top100_dict[key]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}